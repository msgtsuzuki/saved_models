{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3OES2WuAxv8"
      },
      "source": [
        "**Filled notebook:**\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/msgtsuzuki/saved_models/blob/main/tutorial04/Tutorial04-N.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/msgtsuzuki/saved_models/blob/main/tutorial04/Tutorial04-N.ipynb)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvuR0TGcAxwA"
      },
      "source": [
        "# Tutorial 4: Inception, ResNet and DenseNet\n",
        "\n",
        "This tutorial will focus on implementing and examining variations of contemporary CNN architectures. Over the last few years, several architectures have been introduced, some of the most influential and enduring being [GoogleNet Inception](https://arxiv.org/abs/1409.4842) architecture (the winner of ILSVRC 2014), [ResNet](https://arxiv.org/abs/1512.03385) (the winner of ILSVRC 2015) and [DenseNet](https://arxiv.org/abs/1608.06993) (recipient of the best paper award at CVPR 2017). These models were cutting-edge at the time of their introduction, and their fundamental concepts form the basis for many current state-of-the-art architectures. Thus, it is essential to dive into these architectures comprehensively and grasp how to put them into practice. The process begins with the import of the standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o_4pvmNEAxwA"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from types import SimpleNamespace\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib_inline\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGFTVtGPAxwB"
      },
      "source": [
        "The same `set_seed` function will be used as in the previous tutorials, as well as the path variables `DATSET_PATH` and `CHEKPOINT_PATH`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "l5bSU238AxwB"
      },
      "outputs": [],
      "source": [
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial4\"\n",
        "\n",
        "# Function for setting the seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQx_2vaeAxwB"
      },
      "source": [
        "Pretrained models and Tensorboards (further details on this will be provided later) for this tutorial are accessible and can be obtained as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "a9hzJhyaAxwB"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://github.com/msgtsuzuki/saved_models/raw/main/tutorial04/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"GoogleNet.ckpt\", \"ResNet.ckpt\", \"ResNetPreAct.ckpt\", \"DenseNet.ckpt\",\n",
        "                    \"tensorboards/GoogleNet/events.out.tfevents.googlenet\",\n",
        "                    \"tensorboards/ResNet/events.out.tfevents.resnet\",\n",
        "                    \"tensorboards/ResNetPreAct/events.out.tfevents.resnetpreact\",\n",
        "                    \"tensorboards/DenseNet/events.out.tfevents.densenet\"]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsjJ91qXAxwB"
      },
      "source": [
        "Throughout this tutorial, the models will be trained and evaluated on the CIFAR10 dataset. This allows one to compare the results obtained here with the model they have implemented in the first assignment. As has been learned from the previous tutorial on initialization, it is important to have the data preprocessed with a zero mean. Therefore, as a first step, the mean and standard deviation of the CIFAR dataset will be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TNLUw6BsAxwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Data mean [0.49139968 0.48215841 0.44653091]\n",
            "Data std [0.24703223 0.24348513 0.26158784]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
        "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
        "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
        "print(\"Data mean\", DATA_MEANS)\n",
        "print(\"Data std\", DATA_STD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NArGL_kwAxwC"
      },
      "source": [
        "This information is utilized to define a `transforms.Normalize` module which normalizes data accordingly. Additionally, data augmentation is employed during training to mitigate the risk of overfitting and improve the generalization capabilities of CNNs. Specifically, two random augmentations are applied.\n",
        "\n",
        "First, each image is flipped horizontally with a 50\\% probability using `transforms.RandomHorizontalFlip`. The object class typically remains unchanged when an image is flipped and image information is not anticipated to rely on horizontal orientation. However, this might differ if the task involved detecting digits or letters in an image, given their specific orientation.\n",
        "\n",
        "Second, the augmentation known as `transforms.RandomResizedCrop` is utilized. This transformation crops the image within a minor range, potentially altering the aspect ratio, and then scales it back to its original size. As a result, while the actual pixel values are altered, the content or overall semantics of the image remains consistent.\n",
        "\n",
        "The training dataset is randomly divided into a training set and a validation set. The validation set serves the purpose of determining early stopping. Upon completion of training, the models are evaluated on the CIFAR test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k84hw15IAxwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop((32,32), scale=(0.8,1.0), ratio=(0.9,1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "# We need to do a little trick because the validation set should not use the augmentation.\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "set_seed(42)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "set_seed(42)\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBndyrkwAxwD"
      },
      "source": [
        "To verify the effectiveness of normalization, printing out the mean and standard deviation of a single batch is conducted. The mean is expected to be close to 0 and the standard deviation close to 1 for each channel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bF182mYEAxwD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch mean tensor([0.0231, 0.0006, 0.0005])\n",
            "Batch std tensor([0.9865, 0.9849, 0.9868])\n"
          ]
        }
      ],
      "source": [
        "imgs, _ = next(iter(train_loader))\n",
        "print(\"Batch mean\", imgs.mean(dim=[0,2,3]))\n",
        "print(\"Batch std\", imgs.std(dim=[0,2,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwXgRkixAxwD"
      },
      "source": [
        "Finally, visualization of a few images from the training set and their appearance after random data augmentation is conducted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qxbEDoKBAxwD"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "NUM_IMAGES = 4\n",
        "images = [train_dataset[idx][0] for idx in range(NUM_IMAGES)]\n",
        "orig_images = [Image.fromarray(train_dataset.data[idx]) for idx in range(NUM_IMAGES)]\n",
        "orig_images = [test_transform(img) for img in orig_images]\n",
        "\n",
        "img_grid = torchvision.utils.make_grid(torch.stack(images + orig_images, dim=0), nrow=4, normalize=True, pad_value=0.5)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Augmentation examples on CIFAR10\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxzzvooxAxwD"
      },
      "source": [
        "## PyTorch[ Lightning]\n",
        "\n",
        "In this notebook and many subsequent ones, the library [`PyTorch Lightning`](https://www.pytorchlightning.ai/) is utilized. `PyTorch Lightning` is a framework that streamlines the code necessary for trainin[g, evaluating, and ]testing a model in PyTorch. It facilitates logging into [`TensorBoard`](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html), a visualization toolkit for machine learning experiments, and automates the saving of model checkpoints with minimal code overhead. This proves immensely beneficial for focusing on the implementation of different model architectures while minimizing time spent on other coding tasks. It is noted that at the time of writing/teaching, the framework has been released in version 1.8. Future versions may present a slightly altered interface, potentially affecting compatibility with the code (efforts will be made to keep it as up-to-date aspossible).\n",
        "\n",
        "The first step with `PyTorch Lightning` is undertaken, and the exploration of the framework will continue in other tutorials. The library is imported as the initial step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yClW47htAxwD"
      },
      "outputs": [],
      "source": [
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.5\n",
        "    import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiIsfx1mAxwD"
      },
      "source": [
        "PyTorch Lightning comes with a lot of useful functions, such as one for setting the seed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VBGPK9h4AxwD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setting the seed\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwpWnRxUAxwE"
      },
      "source": [
        "Thus, in the future, the definition of a custom `set_seed` function becomes unnecessary.\n",
        "\n",
        "In `PyTorch Lightning`, `pl.LightningModule`s (inheriting from `torch.nn.Module`) are defined, organizing code into five main sections:\n",
        "\n",
        "*  Initialization (`__init__`), where all necessary parameters/models are created.\n",
        "*  Optimizers (`configure_optimizers`) where optimizers, learning rate schedulers, etc., are created.\n",
        "*  Training loop (`training_step`) where the loss calculation for a single batch is defined (the loop of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as well as any logging/saving operation, is performed in the background).\n",
        "*  Validation loop (`validation_step`) where, similar to training, what should happen per step is defined.\n",
        "*  Test loop (`test_step`) which is analogous to validation, only applied to a test set.\n",
        "\n",
        "Hence, the PyTorch code is not abstracted but rather organized and some default operations commonly used are defined. If there is a need to modify anything in the training/validation/test loop, numerous functions are available for override (refer to the [`docs`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html) for details).\n",
        "\n",
        "An example of a Lightning Module for training a CNN is now presented:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BVzKdq0sAxwE"
      },
      "outputs": [],
      "source": [
        "class CIFARModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
        "            model_hparams - Hyperparameters for the model, as dictionary.\n",
        "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
        "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
        "        self.save_hyperparameters()\n",
        "        # Create model\n",
        "        self.model = create_model(model_name, model_hparams)\n",
        "        # Create loss module\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "        # Example input for visualizing the graph in Tensorboard\n",
        "        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        # Forward function that is run when visualizing the graph\n",
        "        return self.model(imgs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # We will support Adam or SGD as optimizers.\n",
        "        if self.hparams.optimizer_name == \"Adam\":\n",
        "            # AdamW is Adam with a correct implementation of weight decay (see here for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
        "            optimizer = optim.AdamW(\n",
        "                self.parameters(), **self.hparams.optimizer_hparams)\n",
        "        elif self.hparams.optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
        "        else:\n",
        "            assert False, f\"Unknown optimizer: \\\"{self.hparams.optimizer_name}\\\"\"\n",
        "\n",
        "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[100, 150], gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # \"batch\" is the output of the training data loader.\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = self.loss_module(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
        "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss  # Return tensor to call \".backward\" on\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs).argmax(dim=-1)\n",
        "        acc = (labels == preds).float().mean()\n",
        "        # By default logs it per epoch (weighted average over batches)\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs).argmax(dim=-1)\n",
        "        acc = (labels == preds).float().mean()\n",
        "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Vz2MOBAxwE"
      },
      "source": [
        "The code is organized and clear, which enables comprehension by others.\n",
        "\n",
        "Another crucial aspect of `PyTorch Lightning` involves the concept of callbacks. Callbacks are self-contained functions embodying the non-essential logic of the Lightning Module. They are typically invoked upon completing a training epoch, but they can also impact other segments of the training loop. For example, the use of two predefined callbacks, `LearningRateMonitor` and `ModelCheckpoint`, is planned. The `LearningRateMonitor` adds the current learning rate to TensorBoard, helping to verify the effective operation of the learning rate scheduler. The `ModelCheckpoint` callback permits customization of the checkpoint saving routine, such as the number of checkpoints to retain, the time of saves and which metric to monitor. These are imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DgnYnPE6AxwE"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlU8C05mAxwE"
      },
      "source": [
        "To facilitate the execution of multiple different models with the same Lightning module, a function is defined below that maps a model name to the model class. At this stage, the dictionary `model_dict` is empty, but it will be populated throughout the notebook with new models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bqSCFCYwAxwE"
      },
      "outputs": [],
      "source": [
        "model_dict = {}\n",
        "\n",
        "def create_model(model_name, model_hparams):\n",
        "    if model_name in model_dict:\n",
        "        return model_dict[model_name](**model_hparams)\n",
        "    else:\n",
        "        assert False, f\"Unknown model name \\\"{model_name}\\\". Available models are: {str(model_dict.keys())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnUtvcmSAxwE"
      },
      "source": [
        "Similarly, to treat the activation function as another hyperparameter in the model, a \"name to function\" dictionary is defined below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LIs-cJ9EAxwE"
      },
      "outputs": [],
      "source": [
        "act_fn_by_name = {\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"leakyrelu\": nn.LeakyReLU,\n",
        "    \"gelu\": nn.GELU\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwKuZBKpAxwE"
      },
      "source": [
        "If classes or objects are passed directly as an argument to the Lightning module, the automatic hyperparameter saving and loading feature of `PyTorch Lightning` could not be used.\n",
        "\n",
        "Beyond the Lightning module, the Trainer constitutes the second pivotal module within `PyTorch Lightning`. The trainer is tasked with executing the training steps delineated in the Lightning module, thus rounding out the framework. Analogously to the Lightning module, any key component deemed unsuitable for automation can be overridden, though default settings frequently represent best practices. For a complete overview, refer to [`documentation`](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html). The principal functions used are as follows:\n",
        "\n",
        "* `trainer.fit`: Accepts a Lightning module, a training dataset, and an (optional) validation dataset as inputs. This function trains the provided module on the training dataset, with occasional validation (by default once per epoch, though this can be modified).\n",
        "* `trainer.test`: Receives a model and a dataset for testing purposes. It outputs the test metric for the dataset.\n",
        "\n",
        "For training and testing, concerns such as setting the model to eval mode (`model.eval()`) are managed automatically. The definition of a training function for the models is illustrated below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Jcss6rgjAxwE"
      },
      "outputs": [],
      "source": [
        "def train_model(model_name, save_name=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
        "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
        "    \"\"\"\n",
        "    if save_name is None:\n",
        "        save_name = model_name\n",
        "\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",                     # We run on a GPU (if possible)\n",
        "                         devices=1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)\n",
        "                         max_epochs=180,                                                                     # How many epochs to train for if no patience is set\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
        "                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n",
        "                         enable_progress_bar=True)                                                           # Set to False if you do not want a progress bar\n",
        "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        model = CIFARModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
        "    else:\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = CIFARModule(model_name=model_name, **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = CIFARModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KizAo8LsAxwE"
      },
      "source": [
        "Finally, focus is shifted to the Convolutional Neural Networks planned for this tutorial: GoogleNet, ResNet, and DenseNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcAwHsL6AxwF"
      },
      "source": [
        "## Inception\n",
        "\n",
        "The [`GoogleNet`](https://arxiv.org/abs/1409.4842), introduced in 2014, clinched the ImageNet Challenge through its implementation of Inception modules. The focus within this tutorial predominantly lies on the Inception concept rather than the specifics of GoogleNet, as Inception has spurred numerous subsequent developments [`Inception-v2`](https://arxiv.org/abs/1512.00567), [`Inception-v3`](https://arxiv.org/abs/1512.00567), [`Inception-v4`](https://arxiv.org/abs/1602.07261), [`Inception-ResNet`](https://arxiv.org/abs/1602.07261), etc. These developments chiefly aim at enhancing efficiency and facilitating the construction of very deep Inception networks. Nonetheess, for a foundational comprehension, examining the riginal Inception block suffices.\n",
        "\n",
        "An Inception bl[ck oncurrently executes four convolutional operations on an identical feature map: $1 \\times 1$, $3 \\times 3$, and $5 \\times 5$ convolutions, along with a max pooling operation. This arrangement enables the network to analyze the same data through different receptive fields. Although a sole reliance on $5 \\times 5$ convolutions could theoretically offer greater capability, it significantly increases both computation and memory demands and increases the risk of overfitting. The general Inception block is depicted as follows (figure credit - [Szegedy et al.](https://arxiv.org/abs/1409.4842)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/inception_block.svg\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"500px\"/></center>\n",
        "\n",
        "The inclusion of $1 \\times 1$ convolutions prior to the $3 \\times 3$ and $5 \\times 5$ convolutions serves the purpose of dimensionality reduction. This step is particularly vital because the feature maps from all branches are combined subsequently, and an explosion in feature size is undesirable. Given that 5x5 convolutions are 25 times more computationally intensive than 1x1 convolutions, a significant reduction in computation and parameters can be achieved by decreasing the dimensionality before undertaking the larger convolutions.\n",
        "\n",
        "An attempt is now made to implement the Inception Block independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_FQyFpzMAxwF"
      },
      "outputs": [],
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_red : dict, c_out : dict, act_fn):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Number of input feature maps from the previous layers\n",
        "            c_red - Dictionary with keys \"3x3\" and \"5x5\" specifying the output of the dimensionality reducing 1x1 convolutions\n",
        "            c_out - Dictionary with keys \"1x1\", \"3x3\", \"5x5\", and \"max\"\n",
        "            act_fn - Activation class constructor (e.g. nn.ReLU)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 1x1 convolution branch\n",
        "        self.conv_1x1 = nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_out[\"1x1\"], kernel_size=1),\n",
        "            nn.BatchNorm2d(c_out[\"1x1\"]),\n",
        "            act_fn()\n",
        "        )\n",
        "\n",
        "        # 3x3 convolution branch\n",
        "        self.conv_3x3 = nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_red[\"3x3\"], kernel_size=1),\n",
        "            nn.BatchNorm2d(c_red[\"3x3\"]),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_red[\"3x3\"], c_out[\"3x3\"], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(c_out[\"3x3\"]),\n",
        "            act_fn()\n",
        "        )\n",
        "\n",
        "        # 5x5 convolution branch\n",
        "        self.conv_5x5 = nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_red[\"5x5\"], kernel_size=1),\n",
        "            nn.BatchNorm2d(c_red[\"5x5\"]),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_red[\"5x5\"], c_out[\"5x5\"], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(c_out[\"5x5\"]),\n",
        "            act_fn()\n",
        "        )\n",
        "\n",
        "        # Max-pool branch\n",
        "        self.max_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
        "            nn.Conv2d(c_in, c_out[\"max\"], kernel_size=1),\n",
        "            nn.BatchNorm2d(c_out[\"max\"]),\n",
        "            act_fn()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_1x1 = self.conv_1x1(x)\n",
        "        x_3x3 = self.conv_3x3(x)\n",
        "        x_5x5 = self.conv_5x5(x)\n",
        "        x_max = self.max_pool(x)\n",
        "        x_out = torch.cat([x_1x1, x_3x3, x_5x5, x_max], dim=1)\n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo1qsPJCAxwF"
      },
      "source": [
        "The `GoogleNet` architecture is characterized by the sequential arrangement of multiple Inception blocks, interspersed with occasional max-pooling to diminish the height and width of the feature maps. Originally tailored for the image dimensions of ImageNet ($224 \\times 224$ pixels), the original `GoogleNet` encompassed nearly 7 million parameters. Given that training in CIFAR10 involves images of $32 \\times 32$ size, a less complex architecture is used instead, featuring a reduced version. The details, such as the number of channels for dimensionality reduction and the output per filter ($1 \\times 1$, $3 \\times 3$, $5 \\times 5$, and max pooling), are subject to manual specification and can be adjusted as desired. It is generally advised to allocate the majority of filters to the $3 \\times 3$ convolutions, considering their efficiency in contextual processing while consuming roughly a third of the parameters required by the 5x5 convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_IFQgvL1AxwF"
      },
      "outputs": [],
      "source": [
        "class GoogleNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10, act_fn_name=\"relu\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.hparams = SimpleNamespace(num_classes=num_classes,\n",
        "                                       act_fn_name=act_fn_name,\n",
        "                                       act_fn=act_fn_by_name[act_fn_name])\n",
        "        self._create_network()\n",
        "        self._init_params()\n",
        "\n",
        "    def _create_network(self):\n",
        "        # A first convolution on the original image to scale up the channel size\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            self.hparams.act_fn()\n",
        "        )\n",
        "        # Stacking inception blocks\n",
        "        self.inception_blocks = nn.Sequential(\n",
        "            InceptionBlock(64, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8}, act_fn=self.hparams.act_fn),\n",
        "            InceptionBlock(64, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12}, act_fn=self.hparams.act_fn),\n",
        "            nn.MaxPool2d(3, stride=2, padding=1),  # 32x32 => 16x16\n",
        "            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12}, act_fn=self.hparams.act_fn),\n",
        "            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn),\n",
        "            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn),\n",
        "            InceptionBlock(96, c_red={\"3x3\": 32, \"5x5\": 16}, c_out={\"1x1\": 32, \"3x3\": 48, \"5x5\": 24, \"max\": 24}, act_fn=self.hparams.act_fn),\n",
        "            nn.MaxPool2d(3, stride=2, padding=1),  # 16x16 => 8x8\n",
        "            InceptionBlock(128, c_red={\"3x3\": 48, \"5x5\": 16}, c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn),\n",
        "            InceptionBlock(128, c_red={\"3x3\": 48, \"5x5\": 16}, c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16}, act_fn=self.hparams.act_fn)\n",
        "        )\n",
        "        # Mapping to classification output\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, self.hparams.num_classes)\n",
        "        )\n",
        "\n",
        "    def _init_params(self):\n",
        "        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, nonlinearity=self.hparams.act_fn_name)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_net(x)\n",
        "        x = self.inception_blocks(x)\n",
        "        x = self.output_net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DvYQbdeAxwF"
      },
      "source": [
        "Now, the model can be integrated into the model dictionary that was defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BJykOGnEAxwF"
      },
      "outputs": [],
      "source": [
        "model_dict[\"GoogleNet\"] = GoogleNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOHG7cw7AxwF"
      },
      "source": [
        "The training of the model is facilitated by `PyTorch Lightning`, necessitating only the definition of the command to initiate the process. It is noteworthy that the training extends for approximately 200 epochs, a duration which amounts to less than an hour on Snellius's standard GPUs (NVIDIA A100). Utilization of saved models is recommended, and personal model training is encouraged for those interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "2262fbf346d64e5295a1b76bfd6c1330",
            "520d1c0f8c444cf4b21c6d8eec0290c3",
            "3e391b5c3e4c4b338c224698db067563",
            "7458b70c536f48fdb9ab632781d39f9b",
            "40d53818f8af4565b9cda6e7117f6849",
            "484b6b4e3b89467d8da318bc1f71c925",
            "3d531734a5934626891e9e50b7aa929d",
            "26061e383bac43d5a22ef1f75fec2c32",
            "78924c3a27a54c6ba74b6c5dd2b7a78d",
            "a5c42d314b8749c68170d3685183e6dc",
            "aa90f1127922464a82d443acd0e295a9",
            "7ecb956cf5144e85a3200a7ebc707c9f",
            "0c541a72b13e495c8646f690b8852fde",
            "aa32a71215f04f46a84f06a0c2e36120",
            "0290e6b9936e40a1ba0da5eeba5ddd4f",
            "ac16093992494bbba7e9da99e915a938",
            "be33192e820e491e822793e4ba61be54",
            "b891de85808c49edb31ac88f37a4c9cd",
            "66c376c4de7a461eac8fcdf003cfb612",
            "94348efa70f1411da7b9ee19ecb48789",
            "c6411314e4344fe8aad71828776f5941",
            "f9b973a32e784467ac0c1c4bd630fc37"
          ]
        },
        "id": "JslzWr-DAxwF",
        "outputId": "c98d53a1-ed74-4e93-c40e-0ab718e4b54a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial4\\GoogleNet.ckpt, loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint d:\\Disciplinas\\DeepLearning\\saved_models\\tutorial4\\GoogleNet.ckpt`\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57477f37878845deb0f9a7a6b6500430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83edf6cadb34475fa945605a516d028b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "googlenet_model, googlenet_results = train_model(model_name=\"GoogleNet\",\n",
        "                                                 model_hparams={\"num_classes\": 10,\n",
        "                                                                \"act_fn_name\": \"relu\"},\n",
        "                                                 optimizer_name=\"Adam\",\n",
        "                                                 optimizer_hparams={\"lr\": 1e-3,\n",
        "                                                                    \"weight_decay\": 1e-4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_urHgYg0AxwI"
      },
      "source": [
        "The results will be compared later in the notebooks, but they can already be printed here for an initial overview:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "kwuimAzoAxwI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GoogleNet Results {'test': 0.8969999551773071, 'val': 0.9039999842643738}\n"
          ]
        }
      ],
      "source": [
        "print(\"GoogleNet Results\", googlenet_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W83YJIqYAxwI"
      },
      "source": [
        "### Tensorboard log\n",
        "\n",
        "An advantageous feature of `PyTorch Lightning` includes automatic log-in to TensorBoard. For a clearer understanding of TensorBoard's applications, examination of the board generated by `PyTorch Lightning` during GoogleNet training is possible. TensorBoard offers inline functionality for Jupyter notebooks, which is utilized here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OmLj9mqvAxwI"
      },
      "outputs": [],
      "source": [
        "# Load tensorboard extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X5EVxxojAxwJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 15560), started 5:33:24 ago. (Use '!kill 15560' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-51b43c23b0baf738\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-51b43c23b0baf738\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
        "%tensorboard --logdir ../saved_models/tutorial4/tensorboards/GoogleNet/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRPYaA7yAxwJ"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/tensorboard_screenshot_GoogleNet.png\" width=\"1000px\"></center>\n",
        "\n",
        "TensorBoard features organization across multiple tabs, with the scalar tab being a principal focus where the evolution of singular metrics is logged. For instance, training loss, accuracy, learning rate, etc., have been charted. Observing training or validation accuracy underscores the importance of employing a learning rate scheduler, where a reduction in the learning rate markedly enhances training performance. Similarly, a noticeable drop in training loss is observed at this point. However, elevated figures in the training set relative to validation suggest overfitting, a common occurrence in expansive networks.\n",
        "\n",
        "Another tab of interest within TensorBoard is the graph tab, which delineates the network architecture, arranged by building blocks from input to output. Essentially, it illustrates the operations conducted during the forward step of `CIFARModule`. Double clicking on a module opens up, offering an invitation to explore the architecture from a novel angle. The visual representation of the graph aids in verifying that the model functions as intended, ensuring that no layers are omitted in the computation graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Et8GT-zAxwJ"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "The [ResNet paper](https://arxiv.org/abs/1603.05027) is one of the most cited AI papers and has been the foundation for neural networks with more than 1,000 layers. Despite its simplicity, the idea of residual connections is highly effective, as it supports stable gradient propagation through the network. Instead of modeling $x_{l+1}=F(x_{l})$, the model $x_{l+1}=x_{l}+F(x_{l})$ is used, where $F$ is a nonlinear mapping (usually a sequence of NN modules like convolutions, activation functions, and normalizations). If backpropagation is performed on such residual connections, the result is\n",
        "$$\n",
        "\\frac{\\partial x_{l+1}}{\\partial x_{l}} = \\mathbf{I} + \\frac{\\partial F(x_{l})}{\\partial x_{l}}.\n",
        "$$\n",
        "The bias towards the identity matrix guarantees a stable gradient propagation that is less affected by $F$ itself. Many variants of ResNet have been proposed, mainly pertaining to the function $F$, or operations applied on the sum. In this tutorial, two of them are examined: the original ResNet block and the Pre-Activation ResNet block. The blocks are visually compared below:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/resnet_block.svg\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"300px\"/></center>\n",
        "\n",
        "The original ResNet block applies a nonlinear activation function, usually ReLU, after the skip connection. In contrast, the preactivation ResNet block applies the nonlinearity at the beginning of $F$. Both have their advantages and disadvantages. For very deep networks, however, the preactivation ResNet has shown to perform better as the gradient flow is guaranteed to have the identity matrix as calculated above, and is not harmed by any non-linear activation applied to it. For comparison, in this notebook, both ResNet types are implemented as shallow networks.\n",
        "\n",
        "Starting with the original ResNet block, the above visualization already shows which layers are included in $F$. One special case that needs to be handled is when the image dimensions in terms of width and height need to be reduced. The basic ResNet block requires $F(x_{l})$ to be of the same shape as $x_{l}$. Thus, the dimensionality of $x_{l}$ should also be changed before adding to $F(x_{l})$. The original implementation used an identity mapping with stride 2 and padded additional feature dimensions with 0. However, the more common implementation is to use a $1 \\times 1$ convolution with stride 2 as it allows changing the dimensionality of the feature while being efficient in parameter and computation cost. The code for the ResNet block is relatively simple and is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lso0u47lAxwJ"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Number of input features\n",
        "            act_fn - Activation class constructor (e.g. nn.ReLU)\n",
        "            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width\n",
        "            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if not subsample:\n",
        "            c_out = c_in\n",
        "\n",
        "        # Network representing F\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),  # No bias needed as the Batch Norm handles it\n",
        "            nn.BatchNorm2d(c_out),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(c_out)\n",
        "        )\n",
        "\n",
        "        # 1x1 convolution with stride 2 means we take the upper left value, and transform it to new output size\n",
        "        self.downsample = nn.Conv2d(c_in, c_out, kernel_size=1, stride=2) if subsample else None\n",
        "        self.act_fn = act_fn()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        out = z + x\n",
        "        out = self.act_fn(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq3pBtS6AxwJ"
      },
      "source": [
        "The second block implemented is the ResNet pre-activation block. For this, the order of the layers in `self.net` needs to be changed, and no activation function is applied on the output. Additionally, the downsampling operation has to apply a nonlinearity, as well as the input, $x_l$, has not been processed by a non-linearity yet. Hence, the block looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "aUqDnnvTAxwJ"
      },
      "outputs": [],
      "source": [
        "class PreActResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Number of input features\n",
        "            act_fn - Activation class constructor (e.g. nn.ReLU)\n",
        "            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width\n",
        "            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if not subsample:\n",
        "            c_out = c_in\n",
        "\n",
        "        # Network representing F\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_in),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),\n",
        "            nn.BatchNorm2d(c_out),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        # 1x1 convolution can apply non-linearity as well, but not strictly necessary\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_in),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_in, c_out, kernel_size=1, stride=2, bias=False)\n",
        "        ) if subsample else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        out = z + x\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGt-09twAxwJ"
      },
      "source": [
        "Similarly to model selection, a dictionary is defined to create a mapping from string to block class. The string name is used as a hyperparameter value in the model to choose between the ResNet blocks. Any other ResNet block type can also be implemented and added here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xdr9TT9VAxwJ"
      },
      "outputs": [],
      "source": [
        "resnet_blocks_by_name = {\n",
        "    \"ResNetBlock\": ResNetBlock,\n",
        "    \"PreActResNetBlock\": PreActResNetBlock\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvWqrQYIAxwJ"
      },
      "source": [
        "The overall ResNet architecture consists of stacking multiple ResNet blocks, some of which are downsampling the input. When referring to ResNet blocks in the whole network, they are usually grouped by the same output shape. For example, by saying that ResNet has `[3,3,3]` blocks, it means that there are 3 groups of 3 ResNet blocks, where subsampling takes place in the fourth and seventh blocks.  ResNet with `[3,3,3]` blocks on CIFAR10 is visualized below.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/resnet_notation.svg\" width=\"500px\"></center>\n",
        "\n",
        "The three groups operate at resolutions $32 \\times 32$, $16 \\times 16$, and $8 \\times 8$, respectively. The blocks in orange denote ResNet blocks with downsampling. The same notation is used by many other implementations such as in the [torchvision library](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet18) from PyTorch. Thus, the code looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9qiLuHwCAxwJ"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10, num_blocks=[3,3,3], c_hidden=[16,32,64], act_fn_name=\"relu\", block_name=\"ResNetBlock\", **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            num_classes - Number of classification outputs (10 for CIFAR10)\n",
        "            num_blocks - List with the number of ResNet blocks to use. The first block of each group uses downsampling, except the first.\n",
        "            c_hidden - List with the hidden dimensionalities in the different blocks. Usually multiplied by 2 the deeper we go.\n",
        "            act_fn_name - Name of the activation function to use, looked up in \"act_fn_by_name\"\n",
        "            block_name - Name of the ResNet block, looked up in \"resnet_blocks_by_name\"\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert block_name in resnet_blocks_by_name\n",
        "        self.hparams = SimpleNamespace(num_classes=num_classes,\n",
        "                                       c_hidden=c_hidden,\n",
        "                                       num_blocks=num_blocks,\n",
        "                                       act_fn_name=act_fn_name,\n",
        "                                       act_fn=act_fn_by_name[act_fn_name],\n",
        "                                       block_class=resnet_blocks_by_name[block_name])\n",
        "        self._create_network()\n",
        "        self._init_params()\n",
        "\n",
        "    def _create_network(self):\n",
        "        c_hidden = self.hparams.c_hidden\n",
        "\n",
        "        # A first convolution on the original image to scale up the channel size\n",
        "        if self.hparams.block_class == PreActResNetBlock: # => Don't apply non-linearity on output\n",
        "            self.input_net = nn.Sequential(\n",
        "                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False)\n",
        "            )\n",
        "        else:\n",
        "            self.input_net = nn.Sequential(\n",
        "                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(c_hidden[0]),\n",
        "                self.hparams.act_fn()\n",
        "            )\n",
        "\n",
        "        # Creating the ResNet blocks\n",
        "        blocks = []\n",
        "        for block_idx, block_count in enumerate(self.hparams.num_blocks):\n",
        "            for bc in range(block_count):\n",
        "                subsample = (bc == 0 and block_idx > 0) # Subsample the first block of each group, except the very first one.\n",
        "                blocks.append(\n",
        "                    self.hparams.block_class(c_in=c_hidden[block_idx if not subsample else (block_idx-1)],\n",
        "                                             act_fn=self.hparams.act_fn,\n",
        "                                             subsample=subsample,\n",
        "                                             c_out=c_hidden[block_idx])\n",
        "                )\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # Mapping to classification output\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c_hidden[-1], self.hparams.num_classes)\n",
        "        )\n",
        "\n",
        "    def _init_params(self):\n",
        "        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n",
        "        # Fan-out focuses on the gradient distribution, and is commonly used in ResNets\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=self.hparams.act_fn_name)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_net(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.output_net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-SdaoifAxwL"
      },
      "source": [
        "The new ResNet class also needs to be added to the model dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "FFTLyBQVAxwL"
      },
      "outputs": [],
      "source": [
        "model_dict[\"ResNet\"] = ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joN5GxPrAxwL"
      },
      "source": [
        "Finally, the ResNet models can be trained. One difference from GoogleNet training is that SGD with Momentum is explicitly used as an optimizer instead of Adam. Adam often leads to slightly worse accuracy on plain, shallow ResNets. The reason for this is not entirely clear, but one possible explanation is related to ResNet's loss surface. ResNet has been shown to produce smoother loss surfaces than networks without skip connections (see [Li et al., 2018](https://arxiv.org/pdf/1712.09913.pdf) for details). A possible visualization of the loss surface with/without skip connections is given below.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial4/resnet_loss_surface.svg\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"600px\"/></center>\n",
        "\n",
        "The $x$ and $y$ axes show a projection of the parameter space, and the $z$ axis shows the loss values achieved by different parameter values. On smooth surfaces like the one on the right, an adaptive learning rate may not be required, as Adam provides. Instead, Adam can get stuck in local optima while SGD finds the wider minima that tend to generalize better.\n",
        "However, to answer this question in detail, an additional tutorial would be needed because it is not easy to answer. For now, the conclusion is as follows: for ResNet architectures, consider the optimizer to be an important hyperparameter and try training with both Adam and SGD. The model can be trained below with SGD:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "OyJ-v3zLAxwL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint d:\\Disciplinas\\DeepLearning\\saved_models\\tutorial4\\ResNet.ckpt`\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial4\\ResNet.ckpt, loading...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8755b07258124ac88ec0e89dd35c8af1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0780637ba5324cb9aa91bbf7b2ef0fcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "resnet_model, resnet_results = train_model(model_name=\"ResNet\",\n",
        "                                           model_hparams={\"num_classes\": 10,\n",
        "                                                          \"c_hidden\": [16,32,64],\n",
        "                                                          \"num_blocks\": [3,3,3],\n",
        "                                                          \"act_fn_name\": \"relu\"},\n",
        "                                           optimizer_name=\"SGD\",\n",
        "                                           optimizer_hparams={\"lr\": 0.1,\n",
        "                                                              \"momentum\": 0.9,\n",
        "                                                              \"weight_decay\": 1e-4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz4RVB0gAxwL"
      },
      "source": [
        "The preactivation ResNet is also trained as a comparison:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RFf5U7fwAxwL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial4\\ResNetPreAct.ckpt, loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint d:\\Disciplinas\\DeepLearning\\saved_models\\tutorial4\\ResNetPreAct.ckpt`\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f089bf3673f8404b84870182cdb71bcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83a1ad3d78a1436ba22437881599309f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "resnetpreact_model, resnetpreact_results = train_model(model_name=\"ResNet\",\n",
        "                                                       model_hparams={\"num_classes\": 10,\n",
        "                                                                      \"c_hidden\": [16,32,64],\n",
        "                                                                      \"num_blocks\": [3,3,3],\n",
        "                                                                      \"act_fn_name\": \"relu\",\n",
        "                                                                      \"block_name\": \"PreActResNetBlock\"},\n",
        "                                                       optimizer_name=\"SGD\",\n",
        "                                                       optimizer_hparams={\"lr\": 0.1,\n",
        "                                                                          \"momentum\": 0.9,\n",
        "                                                                          \"weight_decay\": 1e-4},\n",
        "                                                       save_name=\"ResNetPreAct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQuU_hoIAxwL"
      },
      "source": [
        "### Tensorboard log\n",
        "\n",
        "Similarly to the GoogleNet model, there is also a TensorBoard log for the ResNet model. It can be opened in the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2saxg_vwAxwL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6007 (pid 5292), started 4:50:23 ago. (Use '!kill 5292' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH! Feel free to change \"ResNet\" to \"ResNetPreAct\"\n",
        "%tensorboard --logdir ../saved_models/tutorial4/tensorboards/ResNet/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlVDvA5tAxwM"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/tensorboard_screenshot_ResNet.png\" width=\"1000px\"></center>\n",
        "\n",
        "Feel free to explore the TensorBoard, including the computation graph. In general, it can be observed that with SGD, ResNet has a higher training loss than GoogleNet in the first stage of training. After reducing the learning rate, however, the model achieves even higher validation accuracies. The precise scores at the end of the notebook are compared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34fDWJ6_AxwM"
      },
      "source": [
        "## DenseNet\n",
        "\n",
        "[DenseNet](https://arxiv.org/abs/1608.06993) is another architecture to enable very deep neural networks and takes a slightly different perspective on residual connections. Instead of modeling the difference between layers, DenseNet considers residual connections as a possible way to reuse features across layers, removing any need to learn redundant feature maps. When going deeper into the network, the model learns abstract features to recognize patterns. However, some complex patterns consist of a combination of abstract features (e.g. hand, face, etc.) and low-level features (e.g., edges, basic color, etc.). To find these low-level features in the deep layers, standard CNNs have to learn to copy such feature maps, which wastes a lot of parameter complexity. DenseNet provides an efficient way of reusing features by having each convolution depend on all previous input features, but adds only a small amount of filters to it. See [Figure](https://arxiv.org/abs/1608.06993) for an illustration:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/densenet_block.svg\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"500px\"/></center>\n",
        "\n",
        "The last layer, called the transition layer, is responsible for reducing the dimensionality of the feature maps in height, width, and channel size. Although those technically break the identity backpropagation, there are only a few in a network so that it does not affect the gradient flow much.\n",
        "\n",
        "The implementation of layers in DenseNet is split into three parts: a `DenseLayer`, a `DenseBlock`, and a `TransitionLayer`. The module `DenseLayer` implements a single layer inside a dense block. It applies a $1 \\times 1$ convolution for dimensionality reduction with a subsequent $3 \\times 3$ convolution. The output channels are concatenated into the originals and returned. Note that Batch Normalization is applied as the first layer of each block, allowing slightly different activations for the same features to different layers, depending on what is needed. In general, it can be implemented as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "HTMeDeC7AxwM"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, bn_size, growth_rate, act_fn):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Number of input channels\n",
        "            bn_size - Bottleneck size (factor of growth rate) for the output of the 1x1 convolution. Typically between 2 and 4.\n",
        "            growth_rate - Number of output channels of the 3x3 convolution\n",
        "            act_fn - Activation class constructor (e.g. nn.ReLU)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_in),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_in, bn_size * growth_rate, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(bn_size * growth_rate),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        out = torch.cat([out, x], dim=1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgHBtdc0AxwM"
      },
      "source": [
        "The module `DenseBlock` summarizes multiple dense layers applied in sequence. Each dense layer takes as input the original input concatenated with all previous layers' feature maps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2MXUqeqNAxwM"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, num_layers, bn_size, growth_rate, act_fn):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Number of input channels\n",
        "            num_layers - Number of dense layers to apply in the block\n",
        "            bn_size - Bottleneck size to use in the dense layers\n",
        "            growth_rate - Growth rate to use in the dense layers\n",
        "            act_fn - Activation function to use in the dense layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for layer_idx in range(num_layers):\n",
        "            layers.append(\n",
        "                DenseLayer(c_in=c_in + layer_idx * growth_rate, # Input channels are original plus the feature maps from previous layers\n",
        "                           bn_size=bn_size,\n",
        "                           growth_rate=growth_rate,\n",
        "                           act_fn=act_fn)\n",
        "            )\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSVmhCnNAxwM"
      },
      "source": [
        "Finally, the `TransitionLayer` takes as input the final output of a dense block and reduces its channel dimensionality using a $1 \\times 1$ convolution. To reduce the height and width dimension, a slightly different approach is taken than in ResNet, applying average pooling with kernel size 2 and stride 2. This is because there is no additional connection to the output that would consider the entire $2 \\times 2$ patch instead of a single value. Besides, it is more parameter-efficient than using a $3 \\times 3$ convolution with stride 2. Thus, the layer is implemented as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WduuNQVzAxwM"
      },
      "outputs": [],
      "source": [
        "class TransitionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out, act_fn):\n",
        "        super().__init__()\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_in),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_in, c_out, kernel_size=1, bias=False),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2) # Average the output for each 2x2 pixel group\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transition(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "watjG749AxwM"
      },
      "source": [
        "Now, everything can be put together to create DenseNet. To specify the number of layers, a similar notation as in ResNets is used, passing on a list of ints representing the number of layers per block. After each dense block except the last one, a transition layer is applied to reduce the dimensionality by 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Ip4SfZE3AxwM"
      },
      "outputs": [],
      "source": [
        "class DenseNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10, num_layers=[6,6,6,6], bn_size=2, growth_rate=16, act_fn_name=\"relu\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.hparams = SimpleNamespace(num_classes=num_classes,\n",
        "                                       num_layers=num_layers,\n",
        "                                       bn_size=bn_size,\n",
        "                                       growth_rate=growth_rate,\n",
        "                                       act_fn_name=act_fn_name,\n",
        "                                       act_fn=act_fn_by_name[act_fn_name])\n",
        "        self._create_network()\n",
        "        self._init_params()\n",
        "\n",
        "    def _create_network(self):\n",
        "        c_hidden = self.hparams.growth_rate * self.hparams.bn_size # The start number of hidden channels\n",
        "\n",
        "        # A first convolution on the original image to scale up the channel size\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Conv2d(3, c_hidden, kernel_size=3, padding=1) # No batch norm or activation function as done inside the Dense layers\n",
        "        )\n",
        "\n",
        "        # Creating the dense blocks, eventually including transition layers\n",
        "        blocks = []\n",
        "        for block_idx, num_layers in enumerate(self.hparams.num_layers):\n",
        "            blocks.append(\n",
        "                DenseBlock(c_in=c_hidden,\n",
        "                           num_layers=num_layers,\n",
        "                           bn_size=self.hparams.bn_size,\n",
        "                           growth_rate=self.hparams.growth_rate,\n",
        "                           act_fn=self.hparams.act_fn)\n",
        "            )\n",
        "            c_hidden = c_hidden + num_layers * self.hparams.growth_rate # Overall output of the dense block\n",
        "            if block_idx < len(self.hparams.num_layers)-1: # Don't apply transition layer on last block\n",
        "                blocks.append(\n",
        "                    TransitionLayer(c_in=c_hidden,\n",
        "                                    c_out=c_hidden // 2,\n",
        "                                    act_fn=self.hparams.act_fn))\n",
        "                c_hidden = c_hidden // 2\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # Mapping to classification output\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_hidden), # The features have not passed a non-linearity until here.\n",
        "            self.hparams.act_fn(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c_hidden, self.hparams.num_classes)\n",
        "        )\n",
        "\n",
        "    def _init_params(self):\n",
        "        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=self.hparams.act_fn_name)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_net(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.output_net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQC-Lqh2AxwM"
      },
      "source": [
        "The DenseNet is also added to the model dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "2sR1EuJBAxwM"
      },
      "outputs": [],
      "source": [
        "model_dict[\"DenseNet\"] = DenseNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J7KNooxAxwM"
      },
      "source": [
        "Lastly, the network can be trained. In contrast to ResNet, DenseNet does not show any issues with Adam, and hence it is trained with this optimizer. The other hyperparameters are chosen to result in a network with a parameter size similar to that of ResNet and GoogleNet. Commonly, when designing very deep networks, DenseNet is more parameter-efficient than ResNet, while achieving a similar or even better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "tJN4_-kJAxwN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint d:\\Disciplinas\\DeepLearning\\saved_models\\tutorial4\\DenseNet.ckpt`\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial4\\DenseNet.ckpt, loading...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a623b2566af8440ab7f29fcd63ee5565",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80217194b2904b8db3ed19bf2ac3d265",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "densenet_model, densenet_results = train_model(model_name=\"DenseNet\",\n",
        "                                               model_hparams={\"num_classes\": 10,\n",
        "                                                              \"num_layers\": [6,6,6,6],\n",
        "                                                              \"bn_size\": 2,\n",
        "                                                              \"growth_rate\": 16,\n",
        "                                                              \"act_fn_name\": \"relu\"},\n",
        "                                               optimizer_name=\"Adam\",\n",
        "                                               optimizer_hparams={\"lr\": 1e-3,\n",
        "                                                                  \"weight_decay\": 1e-4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcbCa2pxAxwN"
      },
      "source": [
        "### Tensorboard log\n",
        "\n",
        "Finally, another TensorBoard for DenseNet training is available. It can be explored in the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "PKQWDJe2AxwN",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6008 (pid 5664), started 4:48:26 ago. (Use '!kill 5664' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-3eb13b9046685257\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-3eb13b9046685257\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6008;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
        "%tensorboard --logdir ../saved_models/tutorial4/tensorboards/DenseNet/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo-IdtD0AxwN"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/msgtsuzuki/saved_models/main/tutorial04/tensorboard_screenshot_DenseNet.png\" width=\"1000px\"></center>\n",
        "\n",
        "The overall course of the validation accuracy and training loss resemble that of GoogleNet, which is also related to the training of the network with Adam. Feel free to explore the training metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvWJqN11AxwN"
      },
      "source": [
        "## Conclusion and Comparison\n",
        "\n",
        "After discussing each model separately and training all of them, the results can finally be compared. First, let us organize the results of all models in a table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tLLUZ8ZQAxwN"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<!-- Some HTML code to increase font size in the following table -->\n",
              "<style>\n",
              "th {font-size: 120%;}\n",
              "td {font-size: 120%;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<!-- Some HTML code to increase font size in the following table -->\n",
        "<style>\n",
        "th {font-size: 120%;}\n",
        "td {font-size: 120%;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "pXJJmqBJAxwN"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>Model       </th><th>Val Accuracy  </th><th>Test Accuracy  </th><th>Num Parameters  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>GoogleNet   </td><td>90.40%        </td><td>89.70%         </td><td>260,650         </td></tr>\n",
              "<tr><td>ResNet      </td><td>91.84%        </td><td>91.06%         </td><td>272,378         </td></tr>\n",
              "<tr><td>ResNetPreAct</td><td>91.80%        </td><td>91.07%         </td><td>272,250         </td></tr>\n",
              "<tr><td>DenseNet    </td><td>90.72%        </td><td>90.23%         </td><td>239,146         </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tabulate\n",
        "from IPython.display import display, HTML\n",
        "all_models = [\n",
        "    (\"GoogleNet\", googlenet_results, googlenet_model),\n",
        "    (\"ResNet\", resnet_results, resnet_model),\n",
        "    (\"ResNetPreAct\", resnetpreact_results, resnetpreact_model),\n",
        "    (\"DenseNet\", densenet_results, densenet_model)\n",
        "]\n",
        "table = [[model_name,\n",
        "          f\"{100.0*model_results['val']:4.2f}%\",\n",
        "          f\"{100.0*model_results['test']:4.2f}%\",\n",
        "          \"{:,}\".format(sum([np.prod(p.shape) for p in model.parameters()]))]\n",
        "         for model_name, model_results, model in all_models]\n",
        "display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Model\", \"Val Accuracy\", \"Test Accuracy\", \"Num Parameters\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lQnM3MMAxwN"
      },
      "source": [
        "All models perform reasonably well. Simple models achieve considerably lower performance, attributed to the architecture design choice. GoogleNet obtains the lowest performance in the validation and test set, although it is very close to DenseNet. A proper hyperparameter search over all the channel sizes in GoogleNet would likely improve the accuracy of the model to a similar level, but this is also expensive given the large number of hyperparameters. ResNet outperforms both DenseNet and GoogleNet by more than 1\\% on the validation set, while there is a minor difference between both versions, original and preactivation. For shallow networks, the place of the activation function does not seem to be crucial, although papers have reported the contrary for [very deep networks](https://arxiv.org/abs/1603.05027).\n",
        "\n",
        "In general, it can be concluded that `ResNet` represents a straightforward yet effective architecture. Applying the models to more complex tasks with larger images and additional layers within the networks is likely to reveal a more pronounced disparity between `GoogleNet` and skip-connection architectures like `ResNet` and `DenseNet`. For example, a comparison with deeper models on `CIFAR10` can, for ex[ample, be [found](https://github.com/kuangliu/pytorch-cifar). In particular, `DenseNet` surpasses the original `ResNet` in their setup but narrowly trails behind Pre-Activation `ResNet`. The optimal model, a [dual-path network](https://arxiv.org/abs/1707.01629), merges `ResNet` and `DenseNet`, illustrating that both architectures bring distinct benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3HbnpcoAxwN"
      },
      "source": [
        "### Which model should be chosen for a given task?\n",
        "\n",
        "Four different models have been examined. Thus, the question arises: which should be selected for a new task? Generally, starting with `ResNet` is advisable, given its superior performance on the `CIFAR` dataset and its straightforward implementation. Moreover, for the number of parameters selected herein, `ResNet` operates the quickest, as `DenseNet` and `GoogleNet` incorporate many more layers, which are applied sequentially in a basic implementation. However, for particularly challenging tasks, such as semantic segmentation on HD images, more intricate variants of `ResNet` and `DenseNet` are advocated."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0290e6b9936e40a1ba0da5eeba5ddd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6411314e4344fe8aad71828776f5941",
            "placeholder": "​",
            "style": "IPY_MODEL_f9b973a32e784467ac0c1c4bd630fc37",
            "value": " 7/? [00:22&lt;00:00,  0.31it/s]"
          }
        },
        "0c541a72b13e495c8646f690b8852fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be33192e820e491e822793e4ba61be54",
            "placeholder": "​",
            "style": "IPY_MODEL_b891de85808c49edb31ac88f37a4c9cd",
            "value": "Testing: "
          }
        },
        "2262fbf346d64e5295a1b76bfd6c1330": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_520d1c0f8c444cf4b21c6d8eec0290c3",
              "IPY_MODEL_3e391b5c3e4c4b338c224698db067563",
              "IPY_MODEL_7458b70c536f48fdb9ab632781d39f9b"
            ],
            "layout": "IPY_MODEL_40d53818f8af4565b9cda6e7117f6849"
          }
        },
        "26061e383bac43d5a22ef1f75fec2c32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d531734a5934626891e9e50b7aa929d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e391b5c3e4c4b338c224698db067563": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26061e383bac43d5a22ef1f75fec2c32",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78924c3a27a54c6ba74b6c5dd2b7a78d",
            "value": 14
          }
        },
        "40d53818f8af4565b9cda6e7117f6849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "484b6b4e3b89467d8da318bc1f71c925": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520d1c0f8c444cf4b21c6d8eec0290c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_484b6b4e3b89467d8da318bc1f71c925",
            "placeholder": "​",
            "style": "IPY_MODEL_3d531734a5934626891e9e50b7aa929d",
            "value": "Testing DataLoader 0:  35%"
          }
        },
        "66c376c4de7a461eac8fcdf003cfb612": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7458b70c536f48fdb9ab632781d39f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c42d314b8749c68170d3685183e6dc",
            "placeholder": "​",
            "style": "IPY_MODEL_aa90f1127922464a82d443acd0e295a9",
            "value": " 14/40 [00:54&lt;01:41,  0.26it/s]"
          }
        },
        "78924c3a27a54c6ba74b6c5dd2b7a78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ecb956cf5144e85a3200a7ebc707c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c541a72b13e495c8646f690b8852fde",
              "IPY_MODEL_aa32a71215f04f46a84f06a0c2e36120",
              "IPY_MODEL_0290e6b9936e40a1ba0da5eeba5ddd4f"
            ],
            "layout": "IPY_MODEL_ac16093992494bbba7e9da99e915a938"
          }
        },
        "94348efa70f1411da7b9ee19ecb48789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5c42d314b8749c68170d3685183e6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa32a71215f04f46a84f06a0c2e36120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66c376c4de7a461eac8fcdf003cfb612",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94348efa70f1411da7b9ee19ecb48789",
            "value": 1
          }
        },
        "aa90f1127922464a82d443acd0e295a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac16093992494bbba7e9da99e915a938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b891de85808c49edb31ac88f37a4c9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be33192e820e491e822793e4ba61be54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6411314e4344fe8aad71828776f5941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9b973a32e784467ac0c1c4bd630fc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
